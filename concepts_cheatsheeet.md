
#1. Tokenizing: breaks sentences into words
- using nltk : .word_tokenize() : br
#2. Lemmatization: converts words into the lemma to reduce to all canonical(root) words